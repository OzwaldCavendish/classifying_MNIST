---
title: "MNIST Written Character Classification - Pt1"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}

library(tidyverse)
library(caret)
library(Rtsne)
library(scatterplot3d)
library(keras)

# Windows wd command.  I hate that this line is needed.  I miss Python.
#setwd("C:/Users/Martin/Dropbox/Study/Intro_Machine_Learning/coursework")

# Linux wd command.
setwd("/home/study/Dropbox/Study/Intro_Machine_Learning/coursework")

# ------------------------------------------- #
#      Read in all the samples
#      Code obtained from https://gist.github.com/brendano/39760
#      I added some comments
# ------------------------------------------- #

load_mnist <- function(dir_loc) {
  
  # This function specifically reads images stored in this standard format
  load_image_file <- function(filename) {
    ret = list()
    f = file(filename,'rb')
    
    # Get the row and column size data (image dimensions) stored in the header
    readBin(f,'integer',n=1,size=4,endian='big')
    ret$n = readBin(f,'integer',n=1,size=4,endian='big')
    nrow = readBin(f,'integer',n=1,size=4,endian='big')
    ncol = readBin(f,'integer',n=1,size=4,endian='big')
    x = readBin(f,'integer',n=ret$n*nrow*ncol,size=1,signed=F)
    
    # Iteratively create a matrix until the file runs out of data
    ret$x = matrix(x, ncol=nrow*ncol, byrow=T)
    close(f)
    ret
  }
  
  # This reads the labels
  load_label_file <- function(filename) {
    f = file(filename,'rb')
    readBin(f,'integer',n=1,size=4,endian='big')
    n = readBin(f,'integer',n=1,size=4,endian='big')
    y = readBin(f,'integer',n=n,size=1,signed=F)
    close(f)
    y
  }
  
  # Read all the images, assign to global variable
  trainset <<- data.frame( load_image_file(paste(dir_loc, '/train-images.idx3-ubyte', sep="")) )
  testset <<- data.frame( load_image_file(paste(dir_loc, '/t10k-images.idx3-ubyte', sep="")) )
  
  # Read all the images, assign to global variable
  # This first converts to character, adds prefix "char", then converts to factor.
  trainset$y <<- as.factor( sub("^", "chr", as.character( load_label_file(paste(dir_loc, '/train-labels.idx1-ubyte', sep="")) ) ) )
  testset$y <<- as.factor( sub("^", "chr", as.character( load_label_file(paste(dir_loc, '/t10k-labels.idx1-ubyte', sep="")) ) ) )
  
  # In case there was any ordering present in the datasets, shuffle the rows at random
  set.seed(7)   
  
  trainset <<- trainset[sample(nrow(trainset)),]
  testset <<- testset[sample(nrow(testset)),]

}

load_mnist("data")


# I'm going to scale the data
# Easy trick with images (pixel intensity in range 0 - 255)
# Probably not needed for kNN or PCA, but is good for NN's

for(column in colnames(trainset[,2:785])) {
  trainset[column] = trainset[column] / 255.0
  testset[column] = testset[column] / 255.0
}

```


# 1. Introduction

## Machine Learning and Supervised Classification

In machine learning, a supervised classification problem entails fitting an algorithm to map some input features X to a desirable output label Y, ie; training a machine to automatically classify a sample into some category based on available data about it ("Is this customer fraudulent? Is this a picture of a face?").

Labelled data is used to train the ML algorithm, split into training and test data.  In training the algorithm we run the risk of over-fitting, a complex model can become over-reliant on aspects of the data that are just quirks of the particular set.  The result is reduced performance when the trained model is used to classify new data that it's not been exposed to during training.  The solution to this is to hold out a "test" set of labelled data, and use it to evaluate the model's performance on unseen data.

In this example we have a training dataset of 60,000 samples and a test set of 10,000 samples.  We will train different kNN models, and a neural network, on the first 60,000 and evaluate performance of the different models on the last 10,000.   This will be done by first reducing the dimensionality of the data using PCA, and then selecting the best performing hyperparameter k (number of nearest neighbors to consider) through cross-validated training performance before evaluating a final model on the test set.

As seen in the figure below the classes 0-9 for MNIST are roughly balanced - in this case "Accuracy" is a suitable metric for model performance.  If the dataset were unbalanced, "Precision", "Recall" and their average "F1-Score" would be good choices because they can penalise the model for poor performance on important (for humans) minority classes.


```{r}
# Plot class membership of the training and test data
all_ys <- c(as.character(trainset$y), as.character(testset$y))

# Create a table of counts by membership
all_ys_df <- data.frame( table( all_ys ) )

colnames(all_ys_df) <- c("Character", "Frequency")

# Visualise the numbers within each category
pl <- ggplot(dat=all_ys_df, aes(x=Character, y=Frequency)) +
      geom_bar(stat="identity")
pl

```




## The MNIST data

The MNIST character dataset is an industry standard image classification problem.  An important aspect of the data is that the images have been centred and scaled to the characters, reduced to black and white images and the resolution set uniform.  This removes the need for us to engage in complex image normalisation steps before attempting classification, or more complex models like Convolutional Neural Networks that can automate the discovery of the lowest-level features and relevant areas of an image.  ML algorithms applied to this dataset are then left with higher level logic, finding significant edges and pixels to distinguish between characters.

The images are 28 * 28 matrices of pixel intensity, the first 25 images are displayed below.  The values (which range between 0-255 to represent pixel intensity) are scaled to lie between 0 and 1 - this scaling will help the neural network model to converge and is in general a good idea.  More sophisticated methods of scaling values exist but offer no advantage here.



```{r}
# ------------------------------------------- #
#      The first few training images
# ------------------------------------------- #


to.read = file("./data/train-images.idx3-ubyte", "rb")

# Read the header of the training data
header = readBin(to.read, 'integer', n=4, endian="big")

# Retrieve and plot 25 of the images
# (it's a sequential read)

par(mfrow=c(5, 5), mar=c(0, 0, 0, 0))
for(i in 1:25) {
  m = matrix(readBin(to.read, 'integer', size=1, n=28*28, endian="big", signed=F),28,28)
  
  # Invert the image values for plotting
  m <- 255 - m
  
  # visualise
  image(m[,28:1], col=grey.colors(12), zlim=c(0.0, 255.0))
}

# Close the file
close(to.read)
```



# 2.  Dimensionality Reduction

Principal Component Analysis (PCA) works by calculating the axis in the n-dimensional data space along which variance is greatest, and then sequentially finding perpendicular axes with the next greatest variance.  The algorithm can be used for dimensionality reduction, by dropping all but the first few axes embodying the highest percentage of variance within the dataset.  This retains axes along which data is well separated while dropping "noisy" axes lacking clear information (separation of categories).

Common practice is to keep those that embody 80 % of the variance in the data.  Based on a visual inspection of the cumulative variance with dimensions below, 90 % is chosen instead.  This looks to be a good balance between capturing variance and the diminishing returns with more dimensions.  This variance is in the first 86 Principal components, a significant improvement upon the original dimensionality of 784 individual features (pixels).  The choice of number of principal components is another hyperparameter that one might choose to optimise.


```{r}
# ------------------------------------------- #
#      Pre-modelling visualisation
#      Am expected to use "appropriate dimensionality reduction techniques
#      That means Principal Component analysis and t-SNE to me
# ------------------------------------------- #

# First; pca!  Apparently can't use scaling because of zero value columns
# But I effectively scaled when I normalised to between 0-1 didn't I?
train_pca <- prcomp(trainset[,2:785], center=TRUE)

```

```{r}

# How many of the PCA axes are useful?  Assume threshold of 80 % of cumulative variance
select_dimensionality <- function(sdevs, threshold=0.8) {
  
  # Function assumes it receives a descending-ordered list of the standard deviations of a PCA object
  pca_variance <- sdevs ** 2.0  # Variance is, of course, SD to power 2
  cumulative_variance <- cumsum(pca_variance)
  
  num_PCs = sum( cumulative_variance < threshold * sum(pca_variance) )
  
  plot(cumulative_variance[1:200] / sum(pca_variance) ~ seq(1:200), type="l", xlab="Principal Components", ylab= "Cumulative Variance (% total)")
  abline(v=num_PCs, col="red", lty=2)
  abline(h=threshold, col="blue", lty=1)
  text(x=num_PCs+30, y=0.2, labels = paste("PC's selected =", as.character(num_PCs)))
  text(x=40, y=threshold+0.05, labels=paste("Threshold selected =", as.character(threshold)))
  
  return(num_PCs)
  
}

num_PCs <- select_dimensionality(summary(train_pca)$sdev, threshold=0.9)

# Extract the data to a new DF for later use in ML algorithms
# Drop unwanted PC's
trainset.pca <- data.frame(train_pca$x[,1:num_PCs])
trainset.pca$y <- trainset$y

# Finally, apply the PCA transformations to the test data
testset.pca <- data.frame( predict(train_pca, testset[,2:785])[,1:num_PCs] )
testset.pca$y <- testset$y
colnames(testset.pca) <- sub("x.", "PC", colnames(testset.pca))

```


A secondary advantage of PCA is that it becomes possible to plot the relations between high-dimensional data by plotting the first few Principal components, containing the greatest variance:

```{r}

# Visualise first two principal components nicely
pl <- ggplot(dat=trainset.pca, aes(x=PC1, y=PC2, group=y, color=y)) +
      geom_point(size=3, alpha=0.5) +
      scale_color_brewer(palette="Paired")
pl
```

```{r}
# Visualise the first three principal components poorly
# [DN:  If time include legend]
par(mar=c(0, 0, 0, 0)) # without this, complains about margin size
scatterplot3d(x=trainset.pca$PC1, y=trainset.pca$PC2, z=trainset.pca$PC3, pch=23, bg=trainset.pca$y, xlab="PC1", ylab="PC2", zlab="PC3")
```


For merely visualising high-dimensional data, there is also t-SNE (t-Distributed Stochastic Neighbor Embedding).  This is more powerfully able to visualise the separability of data by dynamically mapping the distance between points in the high-dimensional space to a low-dimensional space in a flexible, stochastic way that is itself considered an ML algorithm.  In this case the algorithm is able to find mappings that clearly separate the majority of the data within the two-dimensional plot.  This indicates that plenty of information is present within the dataset to power classification.

The downside of the method is that because it does not create a specific, algorithmic mapping between the high-dimensional space and the low, it cannot be applied to new data or test data.  It can only be applied to a dataset as a whole and and is not reproducible, so it can't be easily used to create inputs for ML algorithms.

```{r}
# Set random seed for reproducibility (stochastic process)
set.seed(7)   

# Second, t-SNE!  Much preferable.
# Can reduce to arbitrary number of dimensions, default is 2
train_tsne <- Rtsne(as.matrix(trainset[,2:785]), num_threads=8)  # Extra threads not used because openMP on Windows sucks

```


```{r}
trainset.tsne <- data.frame(train_tsne$Y)
trainset.tsne$y <- trainset$y

# The t-SNE algorithm directly reduced the number of dimensions to 2
pl <- ggplot(dat=trainset.tsne, aes(x=X1, y=X2, group=y, color=y)) +
  geom_point(size=3, alpha=0.1) +
  scale_color_brewer(palette="Paired")
pl

```



# 3. Classification with k-Nearest Neighbors (k-NN)

k-NN applied to this dataset benefits greatly from scaling and Principal Component Analysis, both of which have already been applied.  What remains is to determine the best value for the hyperparameter k.  For this purpose I'm using the "caret" ML library, which has lots of useful options and functions for performing grid searches over hyperparameters, before reporting and comparing results on multiple metrics.  Here I compare a raft of different potential values for k in a k-NN model.

Once the model's ideal hyperparameters are identified the model is automatically retrained on all of the available training data, in preparation for application to test data.  At this stage the models are evaluated through 5-fold cross-validation rather than creating an additional hold-out validation set, given the low-ish number of training samples and the reasonably fast execution of the algorithm.

The highest predictive accuracy of 0.9748 is obtained with k=3 nearest neighbors.  As a baseline, with 10 ~ balanced classes we could expect a random process to achieve an accuracy of 0.1.   As an aside, the logLoss and AUC are poor guides to performance here because they are calculated from the probabilities of each label for each sample rather than the final classifications, and so for any given sample can appear to improve as a higher k provides better overall probabilities, whilst the final decision/predicted class is in fact wrong [DN:  REFERENCE!!].


```{r}

# Set up the training algorithm (allows for specifying performance measures and cross-validation folds, etc)
# Specified class probabilities, a more comprehensive summary function and 10-fold cross-validation
control <- trainControl(method="cv", number=5, classProbs=TRUE, summaryFunction=multiClassSummary)  

# The metric the algorithm will use to decide the best-performer
metric <- "Accuracy"

# Create the parameterspace to be explored (only one for this algorithm)
param_grid = data.frame(k = c(3, 5, 7, 9, 11))
```

Fitting the model can take a while.
```{r}
# Fit the kNN models
fit.knn <- train(y~., data=trainset.pca, method="knn", metric=metric, trControl=control, tuneGrid=expand.grid(param_grid))
save(fit.knn, file="knn_model.Rdata")
```

```{r}
# For comparison, fit Naive Bayes
# This just doesn't work, because it doesn't handle zero-variance within classes (ie; all values the same within a class for a given predictor).
fit.bay <- train(y~., data=trainset.pca, method="nb", metric=metric, trControl=control)
save(fit.bay, file="bay_model.Rdata")
```

```{r}
# Plot performance of all knn variants
plot(fit.knn, print.thres=0.5, type="S")
```

```{r}
# Lets look at the model results on some key measures of accuracy
fit.knn$results[,1:7]
```




# 4.  A Further Model - Neural Networks

## Implementing a basic neural network

For the fun of it I'm going to implement a simple, fully connected feedforward neural network.  Normally for image analysis problems a more complex Convolutional Neural Network (CNN) is better able to fit to the low-level features useful for classification of messy image data.  In this case, as stated earlier, the images are already centred, grey-scaled and reduced to standardised 28*28 pixel resolution.  In combination with PCA to find optimal low-level features a standard MLP should perform quite well on this task.

I got the intro to Keras for R from this website;  https://blog.rstudio.com/2017/09/05/keras-for-r/

Yes, it's just calling Python in the background.  Yes, that's just embarassing for R.  But ONS insists...

The chunk below one-hot encodes the targets and converts the input to a numeric matrix.  This is basically only needed because the Caret package I was using demanded data frames, so that's the format I set up to import in.  Keras comes with the MNIST dataset pre-formatted but I want to use the data I've already PCA'd and used for kNN for consistency.

```{r}

# Convert the (now character) classes into one-hot-encoded categorical target vars
trainset_y_cat = to_categorical( as.integer( sub("chr", "", trainset.pca$y) ) )
testset_y_cat = to_categorical( as.integer( sub("chr", "", testset.pca$y) ) )

# Convert the input data into a pure number matrix format
trainset_x_mat = as.matrix( trainset.pca[, -ncol(trainset.pca)]) 
testset_x_mat = as.matrix( testset.pca[, -ncol(testset.pca)]) 

```


```{r}
# Tell it to use python 3 to run this stuff
use_python("usr/bin/python3")


# Model encapsulated in function to facilitate creating multiple variants
build_model <- function(hidden_layers=2, n_neurons=c(64, 32), drop_rate=0.2, input_shape) {
  
  # Import sequential model class
  model <- keras_model_sequential()
  
  # Create all the hidden layers
  for(i in seq(1:hidden_layers)) {
  
    if(i == 1) {
      model %>% layer_dense(units=n_neurons[i], activation="relu", input_shape=input_shape)
    } else {
      model %>% layer_dense(units=n_neurons[i], activation="relu")
    }
    
    # Add dropout to every hidden layer
    model %>% layer_dropout(rate=drop_rate)
  }
  
  # Create the final output layer
  model %>% layer_dense(units=10, activation="softmax")
  
  # Set up loss function, optimiser and metric
  model %>% compile(
    loss="categorical_crossentropy",
    optimizer=optimizer_rmsprop(),
    metrics=c("accuracy")
  )
  
  return(model)
}
```



```{r}
# Evaluate the model
model %>% evaluate(testset_x_mat, testset_y_cat)
```

## Build some models

A range of neural networks with different hyperparameters are were tested iteratively, manually refining the number of neurons and the dropout rate for regularisation.  A more effective method in future, especially given the rapidity of training (< 1 minute per model) would be to set up a grid search over the number of neurons in the 1st and 2nd layers, and over the dropout rate.

```{r}

# Fit a variety of NN's

# Start with something oversized
model1 <- build_model(n_neurons = c(256, 64), input_shape=c( dim(trainset.pca)[2] - 1 ))
summary(model1)
history1 <- model1 %>% fit(trainset_x_mat, trainset_y_cat, epochs = 30, batch_size = 64, validation_split = 0.2)

# Shrink second hidden layer, reduce overfitting?
model2 <- build_model(n_neurons = c(256, 32), input_shape=c( dim(trainset.pca)[2] - 1 ))
summary(model2)
history2 <- model2 %>% fit(trainset_x_mat, trainset_y_cat, epochs = 30, batch_size = 64, validation_split = 0.2)

# Shrink it a bit further, to reduce comp time and overfitting
model3 <- build_model(n_neurons = c(128, 32), input_shape=c( dim(trainset.pca)[2] - 1 ))
summary(model3)
history3 <- model3 %>% fit(trainset_x_mat, trainset_y_cat, epochs = 30, batch_size = 64, validation_split = 0.2)

# Try a higher dropout rate
model4 <- build_model(n_neurons = c(256, 64), drop_rate=0.4, input_shape=c( dim(trainset.pca)[2] - 1 ))
summary(model4)
history4 <- model4 %>% fit(trainset_x_mat, trainset_y_cat, epochs = 30, batch_size = 64, validation_split = 0.2)

```


```{r}
# Plot the model's histories to compare

plot(history1)
plot(history2)
plot(history3)
plot(history4)
```

## 5.  Evaluate Models on Test Set

The two candidate models, k-NN with k=5 and the neural network (two hidden layers of 128 and 32 neurons, dropout rate of 0.4) are evaluated on their accuracy in classifying the test data.

```{r}
# Evaluate the best-performing Neural network on the test data
model3 %>% evaluate(testset_x_mat, testset_y_cat)
```

```{r}
# Evaluate the knn model on the test data
knn_pred <- predict(fit.knn, testset.pca)
```

```{r}
# I want a confusion matrix for best-performer!
```


## 6.  Conclusions &  Recommendations

Based on the chosen metric (accuracy) the neural network outperforms the k-NN algorithm with a relative gain of X % [DN:  Fill this in].  An important property beside accuracy is whether the model performs equally well across all classes.  The final confusion matrix indicates that this is the case, meaning the model would not have a bias problem if used "in anger".  The chosen neural network has 2 hidden layers (relu) of 128 and 32 neurons respectively, with a dropout rate on all hidden neurons of 0.4.

If this model were further developed a good option might be to try other types of regularisation besides dropout.  As with Convolutional Neural Networks (CNN's), since the neural network here is modelling a spatial image problem it's possible that dropping features from the first hidden layer especially makes little logical sense.  If some of those neurons are learning simple features/correlations that are useful across all classifications then dropping them would rob the next layer of universal, essential features.  L1 and L2 regularisation may limit overfitting without this issue.

If an entirely new model were to be developed, then dropping this approach and switching to a 

- compare to kNN
- compare to other models in the MNIST results data
- describe current limitations & strategies to improve

